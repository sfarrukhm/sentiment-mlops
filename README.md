# ğŸš€ Sentiment Analysis MLOps Pipeline

An **end-to-end MLOps project** demonstrating model deployment, observability, and performance optimization using **FastAPI**, **AWS S3**, and **GitHub Actions**.

This repository shows how to take an NLP model from **fine-tuning to scalable inference** â€” with dynamic quantization and CI/CD automation for real-world readiness.

---

## ğŸ§  Project Overview

This project serves a fine-tuned **DistilBERT sentiment classifier** through a production-grade API.
It includes model loading from **AWS S3**, optional **quantized inference**, structured **logging**, **load testing**, and **CI/CD automation**.

Quantization reduced average latency by **â‰ˆ60%**, proving the practical value of lightweight model optimization.

| Mode                 | Avg Latency (ms) | P95 Latency (ms) | Improvement   |
| -------------------- | ---------------- | ---------------- | ------------- |
| Without Quantization | 3274.15          | 5912.65          | â€”             |
| With Quantization    | **1302.57**      | **2581.50**      | âœ… ~60% faster |

---

## ğŸ§© Tech Stack

| Category         | Tools Used                                 |
| ---------------- | ------------------------------------------ |
| **Modeling**     | Hugging Face Transformers (DistilBERT)     |
| **Serving**      | FastAPI, Uvicorn                           |
| **Deployment**   | AWS EC2, S3                                |
| **Automation**   | GitHub Actions (CI/CD)                     |
| **Monitoring**   | Custom structured logging (`logs/app.log`) |
| **Testing**      | Pytest                                     |
| **Load Testing** | Async load simulator with aiohttp          |
| **Optimization** | PyTorch Dynamic Quantization               |

---

## âš™ï¸ Key Features

* ğŸ” **Automated model fetch from S3** during startup
* âš™ï¸ **Dynamic quantization toggle** for faster CPU inference
* ğŸ“ˆ **Structured request logging** (latency, client IP, text length, sentiment)
* ğŸ§ª **Pytest-based CI pipeline** for stability
* ğŸŒ **FastAPI endpoint** for real-time predictions
* ğŸ“Š **Load simulator** to measure performance under concurrent requests

---

## ğŸ§° API Usage

### **Health Check**

```bash
GET /ping
```

âœ… Response:

```json
{"status": 200, "quantized": false}
```

### **Predict Endpoint**

```bash
POST /predict
```

**Body:**

```json
{
  "text": "The movie was absolutely fantastic!",
  "quantize": true
}
```

âœ… Response:

```json
{
  "sentiment": "positive",
  "latency_ms": "1320.45",
  "quantized": true
}
```

---

## ğŸ“¦ Project Structure

```
sentiment-mlops/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml              # Continuous Integration: pytest, lint checks
â”‚       â””â”€â”€ cd.yml              # Continuous Deployment: deploy to EC2
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ serve.py                # FastAPI app for serving predictions
â”‚   â””â”€â”€ utils.py                # Model loading, quantization, and inference logic
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ app.log                 # Application logs
â”‚   â”œâ”€â”€ latency-stats.txt       # Performance summary
â”‚   â””â”€â”€ simulator.log           # Load test results
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ load_simulator.py       # Simulates concurrent requests for stress testing
â”‚   â””â”€â”€ analyze_simulator_logs.py # Parses and visualizes latency results
â”‚
â”œâ”€â”€ deploy.sh                   # Shell script for EC2 deployment
â”œâ”€â”€ Makefile                    # Unified dev commands (test, run, deploy)
â”œâ”€â”€ requirements.txt             # Project dependencies
â”œâ”€â”€ test_setup.py                # Basic API and health-check tests
â””â”€â”€ README.md                    # Documentation (youâ€™re reading it!)

```

---

## ğŸ”„ CI/CD Pipeline

GitHub Actions automates:

* âœ… Environment setup (Python + dependencies)
* âœ… Linting & testing via `pytest`
* âœ… Failure alerts on PRs
* âœ… (Optional) Deployment to EC2 after passing tests

This ensures your API is **always tested before merging**, just like real MLOps production pipelines.

---

## ğŸ“Š Load Testing

Run async load simulation to test stability under concurrent requests:

```bash
python scripts/load_simulator.py
```

Results are logged to `logs/simulator.log` and visualized with a latency-over-time plot.

---

## âš¡ Quantization Impact

| Metric               | Without Quantization | With Quantization |
| -------------------- | -------------------- | ----------------- |
| **Min Latency (ms)** | 331.52               | 291.97            |
| **Avg Latency (ms)** | 3274.15              | **1302.57**       |
| **P95 Latency (ms)** | 5912.65              | **2581.50**       |
| **Max Latency (ms)** | 9143.29              | **4830.67**       |

ğŸ‘‰ Demonstrates how **PyTorch dynamic quantization** reduces model size and speeds up inference â€” essential for CPU-based deployments.

---

## ğŸ§ª Run Tests

```bash
make test
```

or

```bash
PYTHONPATH=. pytest -v
```

---

## â˜ï¸ Deployment

The app is designed for **AWS EC2 deployment**.
Model files are fetched from **S3** on first run and cached locally.

```bash
uvicorn app.serve:app --host 0.0.0.0 --port 8000
```

---

## ğŸ§  MLOps Pitch

This project demonstrates a **production-ready NLP deployment pipeline** with performance optimization and automation at its core.
By integrating **AWS**, **FastAPI**, and **GitHub Actions**, it showcases how MLOps turns research models into **reliable, scalable services** â€” cutting latency by **60%** through smart model quantization.

---

## ğŸ‘¨â€ğŸ’» Author

**M. Farrukh Mehmood**  
ğŸ“« [smfarrukhm@gmail.com](mailto:smfarrukhm@gmail.com)  

ğŸ”— [LinkedIn](https://www.linkedin.com/in/sfarrukhm) | [GitHub](https://github.com/sfarrukhm)
